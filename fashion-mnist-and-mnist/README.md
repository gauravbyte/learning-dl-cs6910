- Built a Neural Network with various gradient descent optimisation algorithms such as 
Stochastic Gradient Descent (SGD), Momentum, Nesterov Accelerated Gradient (NAG), RMSprop, Adam, NAdam
from scratch for Fashion-MNIST and MNIST Image Classification.
- Played around with the hyperparameters to see which combination gave best results.
- Received an overall maximum test accuracy of 86.58% for Fashion-MNIST and 97.63% for MNIST dataset.
- To do: use wandb for sweeps
